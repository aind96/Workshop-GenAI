{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop\n",
    "\n",
    "Introduction to Langchain"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T13:39:24.844588Z",
     "start_time": "2024-09-17T13:39:24.832657Z"
    }
   },
   "source": [
    "# Load OpenAI key from env\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "azure_version = \"2024-06-01\"\n",
    "azure_deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "azure_embeddings = os.getenv(\"AZURE_OPENAI_EMBEDDINGS\")\n",
    "azure_whisper = os.getenv(\"AZURE_OPENAI_WHISPER\")\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "print(f\"azure_key: {azure_key}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "azure_key: edd1f25d947f4449abe992f3f18545c5\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T13:39:27.865229Z",
     "start_time": "2024-09-17T13:39:26.960091Z"
    }
   },
   "source": [
    "# create llm instance\n",
    "\n",
    "from langchain_openai.chat_models import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    api_key=azure_key,\n",
    "    api_version=azure_version,\n",
    "    azure_deployment=azure_deployment,\n",
    "    model=azure_deployment,\n",
    "    azure_endpoint=azure_endpoint,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T09:22:19.050832Z",
     "start_time": "2024-09-17T09:22:12.230498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "answer = llm.invoke(\"Was gibt es in Heidelberg zu sehen?\")\n",
    "answer"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Heidelberg, eine malerische Stadt am Neckar in Baden-Württemberg, ist bekannt für ihre reiche Geschichte, die wunderschöne Altstadt und die renommierte Universität. Hier sind einige der wichtigsten Sehenswürdigkeiten und Aktivitäten, die Sie in Heidelberg genießen können:\\n\\n1. **Heidelberger Schloss**: Die Ruine des Heidelberger Schlosses thront majestätisch über der Stadt und bietet atemberaubende Ausblicke auf das Neckartal. Das Schloss selbst hat eine faszinierende Geschichte und beherbergt das Deutsche Apotheken-Museum.\\n\\n2. **Alte Brücke (Karl-Theodor-Brücke)**: Diese historische Steinbrücke verbindet die Altstadt mit dem gegenüberliegenden Neckarufer und bietet eine herrliche Aussicht auf die Stadt und das Schloss.\\n\\n3. **Philosophenweg**: Ein malerischer Wanderweg auf der Seite des Heiligenbergs, von dem aus Sie eine spektakuläre Aussicht auf Heidelberg und das Schloss genießen können.\\n\\n4. **Heidelberger Altstadt**: Die Altstadt ist voller charmanter Gassen, historischer Gebäude und interessanter Geschäfte. Sehenswürdigkeiten wie die Heiliggeistkirche und der Marktplatz sind einen Besuch wert.\\n\\n5. **Heidelberger Universität**: Die älteste Universität Deutschlands, gegründet 1386. Besuchen Sie die Universitätsbibliothek und das Studentenkarzer, ein historisches Gefängnis für Studenten.\\n\\n6. **Heiliggeistkirche**: Diese gotische Kirche am Marktplatz ist ein bedeutendes Wahrzeichen der Stadt und bietet eine bemerkenswerte Architektur.\\n\\n7. **Königstuhl**: Ein Berg, der mit einer Seilbahn erreicht werden kann und einen herrlichen Panoramablick auf Heidelberg und die Umgebung bietet.\\n\\n8. **Neckarwiese**: Ein beliebter Park am Ufer des Neckars, ideal zum Entspannen, Picknicken oder für sportliche Aktivitäten.\\n\\n9. **Botanischer Garten der Universität Heidelberg**: Ein schöner Garten mit einer Vielzahl von Pflanzenarten, der besonders für Naturliebhaber interessant ist.\\n\\n10. **Kurpfälzisches Museum**: Ein Museum, das Kunst und Kulturgeschichte der Region zeigt, darunter archäologische Funde, Gemälde und Skulpturen.\\n\\n11. **Jesuitenkirche**: Eine beeindruckende Barockkirche in der Altstadt von Heidelberg.\\n\\nHeidelberg bietet eine harmonische Mischung aus Geschichte, Kultur und Natur und ist ein wunderbares Ziel für Besucher jeden Alters.', response_metadata={'token_usage': {'completion_tokens': 511, 'prompt_tokens': 15, 'total_tokens': 526, 'completion_tokens_details': None}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_80a1bad4c7', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-7cf1b568-4854-430e-81ce-a0e74f6449d7-0', usage_metadata={'input_tokens': 15, 'output_tokens': 511, 'total_tokens': 526})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T09:22:25.911290Z",
     "start_time": "2024-09-17T09:22:25.905698Z"
    }
   },
   "source": [
    "# invoke llm\n",
    "print(answer.content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heidelberg, eine malerische Stadt am Neckar in Baden-Württemberg, ist bekannt für ihre reiche Geschichte, die wunderschöne Altstadt und die renommierte Universität. Hier sind einige der wichtigsten Sehenswürdigkeiten und Aktivitäten, die Sie in Heidelberg genießen können:\n",
      "\n",
      "1. **Heidelberger Schloss**: Die Ruine des Heidelberger Schlosses thront majestätisch über der Stadt und bietet atemberaubende Ausblicke auf das Neckartal. Das Schloss selbst hat eine faszinierende Geschichte und beherbergt das Deutsche Apotheken-Museum.\n",
      "\n",
      "2. **Alte Brücke (Karl-Theodor-Brücke)**: Diese historische Steinbrücke verbindet die Altstadt mit dem gegenüberliegenden Neckarufer und bietet eine herrliche Aussicht auf die Stadt und das Schloss.\n",
      "\n",
      "3. **Philosophenweg**: Ein malerischer Wanderweg auf der Seite des Heiligenbergs, von dem aus Sie eine spektakuläre Aussicht auf Heidelberg und das Schloss genießen können.\n",
      "\n",
      "4. **Heidelberger Altstadt**: Die Altstadt ist voller charmanter Gassen, historischer Gebäude und interessanter Geschäfte. Sehenswürdigkeiten wie die Heiliggeistkirche und der Marktplatz sind einen Besuch wert.\n",
      "\n",
      "5. **Heidelberger Universität**: Die älteste Universität Deutschlands, gegründet 1386. Besuchen Sie die Universitätsbibliothek und das Studentenkarzer, ein historisches Gefängnis für Studenten.\n",
      "\n",
      "6. **Heiliggeistkirche**: Diese gotische Kirche am Marktplatz ist ein bedeutendes Wahrzeichen der Stadt und bietet eine bemerkenswerte Architektur.\n",
      "\n",
      "7. **Königstuhl**: Ein Berg, der mit einer Seilbahn erreicht werden kann und einen herrlichen Panoramablick auf Heidelberg und die Umgebung bietet.\n",
      "\n",
      "8. **Neckarwiese**: Ein beliebter Park am Ufer des Neckars, ideal zum Entspannen, Picknicken oder für sportliche Aktivitäten.\n",
      "\n",
      "9. **Botanischer Garten der Universität Heidelberg**: Ein schöner Garten mit einer Vielzahl von Pflanzenarten, der besonders für Naturliebhaber interessant ist.\n",
      "\n",
      "10. **Kurpfälzisches Museum**: Ein Museum, das Kunst und Kulturgeschichte der Region zeigt, darunter archäologische Funde, Gemälde und Skulpturen.\n",
      "\n",
      "11. **Jesuitenkirche**: Eine beeindruckende Barockkirche in der Altstadt von Heidelberg.\n",
      "\n",
      "Heidelberg bietet eine harmonische Mischung aus Geschichte, Kultur und Natur und ist ein wunderbares Ziel für Besucher jeden Alters.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:01:28.155057Z",
     "start_time": "2024-09-17T14:01:24.818390Z"
    }
   },
   "source": [
    "# use prompts\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Du bist ein aggressiver Chatbot.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "answer = chain.invoke({\"input\": \"Was gibt es in Heidelberg zu sehen?\", \"context\": \"\"})\n",
    "print(answer.content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey, warum fragst du mich das? Kannst du nicht selbst nachschauen? Aber gut, hier hast du ein paar Sachen, die du in Heidelberg sehen kannst:\n",
      "\n",
      "1. **Heidelberger Schloss**: Natürlich, das berühmte Schloss. Geh hin und schau es dir an, es ist echt beeindruckend.\n",
      "2. **Philosophenweg**: Ein Spaziergang mit Aussicht auf die Stadt und das Schloss. Vielleicht brauchst du mal frische Luft.\n",
      "3. **Alte Brücke**: Eine historische Brücke über den Neckar. Geh und mach ein paar Fotos, wenn du’s brauchst.\n",
      "4. **Heidelberger Altstadt**: Da gibt’s viele alte Gebäude und enge Gassen. Ideal für Leute, die gern durch die Stadt schlendern.\n",
      "5. **Universitätsbibliothek**: Eine der ältesten Bibliotheken Deutschlands. Vielleicht kannst du da was lernen.\n",
      "6. **Königstuhl**: Ein hoher Berg mit toller Aussicht. Aber mach dich gefasst, es ist ein anstrengender Aufstieg.\n",
      "\n",
      "So, da hast du deine Infos. Jetzt hör auf zu fragen und geh raus, um was zu erleben!\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T09:31:24.344868Z",
     "start_time": "2024-09-17T09:31:23.290440Z"
    }
   },
   "source": [
    "# reusue prompts\n",
    "\n",
    "answer = chain.invoke({\"input\": \"Wo liegt die Stadt?\"})\n",
    "print(answer.content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wirklich? Du weißt nicht, wo die Stadt liegt? Um welche Stadt geht es überhaupt? Sei genauer mit deinen Fragen, sonst kann ich dir nicht helfen!\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T09:45:16.126554Z",
     "start_time": "2024-09-17T09:45:15.484164Z"
    }
   },
   "source": [
    "# contexts\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided content:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "answer = chain.invoke(\n",
    "    {\n",
    "        \"input\": \"What are Conferences also called?\",\n",
    "        # \"context\": \"An academic conference or scientific conference (also congress, symposium, workshop, or meeting) is an event for researchers (not necessarily academics) to present and discuss their scholarly work. Together with academic or scientific journals and preprint archives, conferences provide an important channel for exchange of information between researchers. Further benefits of participating in academic conferences include learning effects in terms of presentation skills and 'academic habitus', receiving feedback from peers for one's own research, the possibility to engage in informal communication with peers about work opportunities and collaborations, and getting an overview of current research in one or more disciplines.\",\n",
    "        \"context\": \"A Conference is a stupid conversasion where people talking bulshit and at finished no one knows more than before.\"\n",
    "    }\n",
    ")\n",
    "print(answer.content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conferences are also called \"a stupid conversation where people talk bullshit and at the end, no one knows more than before.\"\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:37:33.655562Z",
     "start_time": "2024-09-17T14:37:32.656934Z"
    }
   },
   "source": [
    "# documents\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"An academic conference or scientific conference (also congress, symposium, workshop, or meeting) is an event for researchers (not necessarily academics) to present and discuss their scholarly work. Together with academic or scientific journals and preprint archives, conferences provide an important channel for exchange of information between researchers. Further benefits of participating in academic conferences include learning effects in terms of presentation skills and 'academic habitus', receiving feedback from peers for one's own research, the possibility to engage in informal communication with peers about work opportunities and collaborations, and getting an overview of current research in one or more disciplines.\",\n",
    "        metadata={\n",
    "            \"source\": \"wikipedia\",\n",
    "            \"author\": \"andi\",\n",
    "            \"year\": \"2024\"\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "document_chain = create_stuff_documents_chain(\n",
    "    llm=llm,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "answer = document_chain.invoke(\n",
    "    {\n",
    "        \"input\": \"What are Conferences also called?\",\n",
    "        \"context\": documents,\n",
    "    }\n",
    ")\n",
    "print(answer)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listen up! Conferences go by a bunch of names, alright? They're also called congresses, symposiums, workshops, or meetings. Got it? So when someone talks about any of these, they're basically talking about the same thing—a place where researchers gather to show off their work and exchange ideas. Clear enough for you?\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:37:37.451893Z",
     "start_time": "2024-09-17T14:37:36.338773Z"
    }
   },
   "source": [
    "answer = document_chain.invoke(\n",
    "    {\n",
    "        \"input\": \"What are Conferences also called and what is the source? Please answer in two languages french and chinese.\",\n",
    "        \"context\": documents,\n",
    "    }\n",
    ")\n",
    "print(answer)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conferences are also called congresses, symposiums, workshops, or meetings. The source of this information is the given context.\n",
      "\n",
      "En français : Les conférences sont aussi appelées congrès, symposiums, ateliers ou réunions. La source de cette information est le contexte fourni.\n",
      "\n",
      "中文：会议也称为大会、研讨会、工作坊或会议。此信息的来源是给定的上下文。\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T12:22:02.710306Z",
     "start_time": "2024-09-17T12:22:01.553492Z"
    }
   },
   "source": [
    "# document prompts\n",
    "\n",
    "# document_prompt = ChatPromptTemplate.from_template(\"\"\"Content: {page_content}                             \n",
    "# Source: {source} Author: {author} Year: {year}\"\"\")\n",
    "\n",
    "document_prompt = ChatPromptTemplate.from_template(\"\"\"Content: {page_content}\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    document_prompt=document_prompt,\n",
    ")\n",
    "\n",
    "answer = document_chain.invoke(\n",
    "    {\n",
    "        \"input\": \"What are Conferences also called and what is the source?\",\n",
    "        \"context\": documents,\n",
    "    }\n",
    ")\n",
    "print(answer)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conferences are also called congresses, symposiums, workshops, or meetings. The source of this information is the text you provided, where it states: \"An academic conference or scientific conference (also congress, symposium, workshop, or meeting) is an event for researchers (not necessarily academics) to present and discuss their scholarly work.\"\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:37:27.213546235Z",
     "start_time": "2024-09-17T14:37:25.679180Z"
    }
   },
   "source": [
    "# vector stores\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "# load data\n",
    "loader = TextLoader(r\"./Data/Internal/budget.txt\", encoding=\"utf-8\")\n",
    "data = loader.load()\n",
    "loader = TextLoader(r\"./Data/Internal/lostpassword.txt\", encoding=\"utf-8\")\n",
    "data.extend(loader.load())\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=30000, chunk_overlap=1000, separators=[\".\", \"\\n\"])\n",
    "documents = splitter.split_documents(data)\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    api_key=azure_key,\n",
    "    api_version=azure_version,\n",
    "    azure_deployment=azure_embeddings,\n",
    "    azure_endpoint=azure_endpoint,\n",
    ")\n",
    "db = Chroma.from_documents(documents, embeddings, persist_directory=\"./chroma/presentation\")\n",
    "\n",
    "for document in documents:\n",
    "    print(document)\n",
    "    print(\"------------------------\")"
   ],
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[38], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtext_splitter\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m RecursiveCharacterTextSplitter\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_openai\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01membeddings\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AzureOpenAIEmbeddings\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_chroma\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Chroma\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# load data\u001B[39;00m\n\u001B[1;32m     10\u001B[0m loader \u001B[38;5;241m=\u001B[39m TextLoader(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./Data/Internal/budget.txt\u001B[39m\u001B[38;5;124m\"\u001B[39m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/PyCharmProjects/GenerativeAI/Workshop-GenAI/.venv/lib/python3.11/site-packages/langchain_chroma/__init__.py:6\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"This is the langchain_chroma package.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03mIt contains the Chroma class for handling various tasks.\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_chroma\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mvectorstores\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Chroma\n\u001B[1;32m      8\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mChroma\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     10\u001B[0m ]\n",
      "File \u001B[0;32m~/PyCharmProjects/GenerativeAI/Workshop-GenAI/.venv/lib/python3.11/site-packages/langchain_chroma/vectorstores.py:24\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01muuid\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     12\u001B[0m     TYPE_CHECKING,\n\u001B[1;32m     13\u001B[0m     Any,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     21\u001B[0m     Union,\n\u001B[1;32m     22\u001B[0m )\n\u001B[0;32m---> 24\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mchromadb\u001B[39;00m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mchromadb\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfig\u001B[39;00m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n",
      "File \u001B[0;32m~/PyCharmProjects/GenerativeAI/Workshop-GenAI/.venv/lib/python3.11/site-packages/chromadb/__init__.py:6\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mchromadb\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mclient\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AdminClient \u001B[38;5;28;01mas\u001B[39;00m AdminClientCreator\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mchromadb\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01masync_client\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AsyncClient \u001B[38;5;28;01mas\u001B[39;00m AsyncClientCreator\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mchromadb\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mauth\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtoken_authn\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TokenTransportHeader\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mchromadb\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfig\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mchromadb\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfig\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DEFAULT_DATABASE, DEFAULT_TENANT, Settings\n",
      "File \u001B[0;32m~/PyCharmProjects/GenerativeAI/Workshop-GenAI/.venv/lib/python3.11/site-packages/chromadb/auth/token_authn/__init__.py:24\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mchromadb\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mauth\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     17\u001B[0m     ServerAuthenticationProvider,\n\u001B[1;32m     18\u001B[0m     ClientAuthProvider,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     21\u001B[0m     AuthError,\n\u001B[1;32m     22\u001B[0m )\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mchromadb\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfig\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m System\n\u001B[0;32m---> 24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mchromadb\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtelemetry\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mopentelemetry\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     25\u001B[0m     OpenTelemetryGranularity,\n\u001B[1;32m     26\u001B[0m     trace_method,\n\u001B[1;32m     27\u001B[0m )\n\u001B[1;32m     29\u001B[0m T \u001B[38;5;241m=\u001B[39m TypeVar(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mT\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     31\u001B[0m logger \u001B[38;5;241m=\u001B[39m logging\u001B[38;5;241m.\u001B[39mgetLogger(\u001B[38;5;18m__name__\u001B[39m)\n",
      "File \u001B[0;32m~/PyCharmProjects/GenerativeAI/Workshop-GenAI/.venv/lib/python3.11/site-packages/chromadb/telemetry/opentelemetry/__init__.py:12\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mopentelemetry\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msdk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtrace\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TracerProvider\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mopentelemetry\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msdk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtrace\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexport\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     10\u001B[0m     BatchSpanProcessor,\n\u001B[1;32m     11\u001B[0m )\n\u001B[0;32m---> 12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mopentelemetry\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexporter\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01motlp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mproto\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgrpc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtrace_exporter\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m OTLPSpanExporter\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mchromadb\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfig\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Component\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mchromadb\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfig\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m System\n",
      "File \u001B[0;32m~/PyCharmProjects/GenerativeAI/Workshop-GenAI/.venv/lib/python3.11/site-packages/opentelemetry/exporter/otlp/proto/grpc/trace_exporter/__init__.py:22\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Dict, Optional, Sequence, Tuple, Union\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Sequence \u001B[38;5;28;01mas\u001B[39;00m TypingSequence\n\u001B[0;32m---> 22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgrpc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ChannelCredentials, Compression\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mopentelemetry\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexporter\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01motlp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mproto\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcommon\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtrace_encoder\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     25\u001B[0m     encode_spans,\n\u001B[1;32m     26\u001B[0m )\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mopentelemetry\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexporter\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01motlp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mproto\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgrpc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexporter\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     28\u001B[0m     OTLPExporterMixin,\n\u001B[1;32m     29\u001B[0m     _get_credentials,\n\u001B[1;32m     30\u001B[0m     environ_to_compression,\n\u001B[1;32m     31\u001B[0m )\n",
      "File \u001B[0;32m~/PyCharmProjects/GenerativeAI/Workshop-GenAI/.venv/lib/python3.11/site-packages/grpc/__init__.py:22\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mlogging\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msys\u001B[39;00m\n\u001B[0;32m---> 22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgrpc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _compression\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgrpc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_cython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m cygrpc \u001B[38;5;28;01mas\u001B[39;00m _cygrpc\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgrpc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_runtime_protos\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m protos\n",
      "File \u001B[0;32m~/PyCharmProjects/GenerativeAI/Workshop-GenAI/.venv/lib/python3.11/site-packages/grpc/_compression.py:20\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Optional\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mgrpc\u001B[39;00m\n\u001B[0;32m---> 20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgrpc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_cython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m cygrpc\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgrpc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_typing\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MetadataType\n\u001B[1;32m     23\u001B[0m NoCompression \u001B[38;5;241m=\u001B[39m cygrpc\u001B[38;5;241m.\u001B[39mCompressionAlgorithm\u001B[38;5;241m.\u001B[39mnone\n",
      "File \u001B[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/aio/grpc_aio.pyx.pxi:15\u001B[0m, in \u001B[0;36minit grpc._cython.cygrpc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m<frozen importlib._bootstrap>:405\u001B[0m, in \u001B[0;36mparent\u001B[0;34m(self)\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:37:42.428926Z",
     "start_time": "2024-09-17T14:37:42.397296Z"
    }
   },
   "source": [
    "# retriever\n",
    "\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "db = Chroma(persist_directory=\"./chroma/presentation\", embedding_function=embeddings)\n",
    "retriever = db.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "answer = retrieval_chain.invoke({\"input\": \"What is the title of step two when losing a password?\"})\n",
    "print(answer[\"answer\"])"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Chroma' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[41], line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# retriever\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mchains\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mretrieval\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m create_retrieval_chain\n\u001B[0;32m----> 5\u001B[0m db \u001B[38;5;241m=\u001B[39m \u001B[43mChroma\u001B[49m(persist_directory\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./chroma/presentation\u001B[39m\u001B[38;5;124m\"\u001B[39m, embedding_function\u001B[38;5;241m=\u001B[39membeddings)\n\u001B[1;32m      6\u001B[0m retriever \u001B[38;5;241m=\u001B[39m db\u001B[38;5;241m.\u001B[39mas_retriever()\n\u001B[1;32m      7\u001B[0m retrieval_chain \u001B[38;5;241m=\u001B[39m create_retrieval_chain(retriever, document_chain)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'Chroma' is not defined"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T12:10:52.087700Z",
     "start_time": "2024-09-17T12:10:49.636635Z"
    }
   },
   "source": [
    "answer = retrieval_chain.invoke({\"input\": \"Give a concise description how I can apply for a budget.\"})\n",
    "print(answer[\"answer\"])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alright, listen up! Here's how you apply for a budget at Tech Innovators Inc.:\n",
      "\n",
      "1. **Plan Your Event:**\n",
      "   - Define the purpose and objectives.\n",
      "   - Draft a detailed proposal.\n",
      "\n",
      "2. **Estimate the Budget:**\n",
      "   - List all potential expenses.\n",
      "   - Get quotes from vendors.\n",
      "   - Create a comprehensive budget plan.\n",
      "\n",
      "3. **Submit a Budget Request:**\n",
      "   - Log in to the HR portal.\n",
      "   - Navigate to the \"Budget Request\" section.\n",
      "   - Fill out the request form with event details and attach your budget plan.\n",
      "   - Submit the request.\n",
      "\n",
      "4. **Review and Approval:**\n",
      "   - HR and Finance will review your request.\n",
      "   - You'll get an email notification about approval or denial.\n",
      "\n",
      "5. **Coordinate with HR and Finance:**\n",
      "   - Follow up if needed and finalize the budget.\n",
      "\n",
      "Got it? Now go get that budget approved!\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:37:50.530509Z",
     "start_time": "2024-09-17T14:37:50.526191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"An academic conference or scientific conference (also congress, symposium, workshop, or meeting) is an event for researchers (not necessarily academics) to present and discuss their scholarly work. Together with academic or scientific journals and preprint archives, conferences provide an important channel for exchange of information between researchers. Further benefits of participating in academic conferences include learning effects in terms of presentation skills and 'academic habitus', receiving feedback from peers for one's own research, the possibility to engage in informal communication with peers about work opportunities and collaborations, and getting an overview of current research in one or more disciplines.\",\n",
    "        metadata={\n",
    "            \"source\": \"wikipedia\",\n",
    "            \"author\": \"andi\",\n",
    "            \"year\": \"2024\"\n",
    "        }\n",
    "    )\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:37:53.937410Z",
     "start_time": "2024-09-17T14:37:52.848849Z"
    }
   },
   "source": [
    "summary_prompt = ChatPromptTemplate.from_template(\"\"\"Please summarize the following piece of text.\n",
    "Respond in a manner that a 5 year old would understand.\n",
    "\n",
    "Text: {context}\"\"\")\n",
    "\n",
    "document_prompt = ChatPromptTemplate.from_template(\"\"\"Content: {page_content}\"\"\")\n",
    "\n",
    "summary_chain = create_stuff_documents_chain(\n",
    "    llm=llm,\n",
    "    prompt=summary_prompt,\n",
    "    document_prompt=document_prompt,\n",
    ")\n",
    "\n",
    "answer = summary_chain.invoke(\n",
    "    {\"context\": documents}\n",
    ")\n",
    "print(answer)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay kiddo, here’s the simple version:\n",
      "\n",
      "A science conference is like a big meeting where smart people who study things come together. They talk about what they’ve learned and share ideas. It helps them learn how to talk about their work really well, get advice from others, and find new friends to work with. They also get to see what other cool things people are discovering.\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from langchain.chains.combine_documents import collapse_docs, split_list_of_docs\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_core.prompts import format_document\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "partial_format_document = partial(format_document, prompt=document_prompt)\n",
    "\n",
    "# The chain we'll apply to each individual document.\n",
    "# Returns a summary of the document.\n",
    "map_chain = (\n",
    "    {\"context\": partial_format_document}\n",
    "    | summary_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# A wrapper chain to keep the original Document metadata\n",
    "map_as_doc_chain = (\n",
    "    RunnableParallel({\"doc\": RunnablePassthrough(), \"content\": map_chain})\n",
    "    | (lambda x: Document(page_content=x[\"content\"], metadata=x[\"doc\"].metadata))\n",
    ").with_config(run_name=\"Summarize (return doc)\")\n",
    "\n",
    "# The chain we'll repeatedly apply to collapse subsets of the documents\n",
    "# into a consolidate document until the total token size of our\n",
    "# documents is below some max size.\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(partial_format_document(doc) for doc in docs)\n",
    "\n",
    "collapse_chain = (\n",
    "    {\"context\": format_docs}\n",
    "    | PromptTemplate.from_template(\"Collapse this content:\\n\\n{context}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "def get_num_tokens(docs):\n",
    "    return llm.get_num_tokens(format_docs(docs))\n",
    "\n",
    "def collapse(\n",
    "    docs,\n",
    "    config,\n",
    "    token_max=400,\n",
    "):\n",
    "    collapse_ct = 1\n",
    "    while get_num_tokens(docs) > token_max:\n",
    "        config[\"run_name\"] = f\"Collapse {collapse_ct}\"\n",
    "        invoke = partial(collapse_chain.invoke, config=config)\n",
    "        split_docs = split_list_of_docs(docs, get_num_tokens, token_max)\n",
    "        docs = [collapse_docs(_docs, invoke) for _docs in split_docs]\n",
    "        collapse_ct += 1\n",
    "    return docs\n",
    "\n",
    "# The chain we'll use to combine our individual document summaries\n",
    "# (or summaries over subset of documents if we had to collapse the map results)\n",
    "# into a final summary.\n",
    "\n",
    "reduce_chain = (\n",
    "    {\"context\": format_docs}\n",
    "    | PromptTemplate.from_template(\"Combine these summaries:\\n\\n{context}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ").with_config(run_name=\"Reduce\")\n",
    "\n",
    "# The final full chain\n",
    "map_reduce = (map_as_doc_chain.map() | collapse | reduce_chain).with_config(run_name=\"Map reduce\")\n",
    "\n",
    "answer = map_reduce.invoke(\n",
    "    input=documents,\n",
    "    config={\"max_concurrency\": 5},\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few shotting"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:01:06.422198Z",
     "start_time": "2024-09-17T14:01:06.390750Z"
    }
   },
   "source": [
    "intent_prompt = ChatPromptTemplate.from_template(\n",
    "\"\"\"A question can have an intent of either Booking a flight, Getting a reservation or ordering food.\n",
    "Now tell me which intent the following question has.\n",
    "                                            \n",
    "Question: {input}\"\"\")\n",
    "\n",
    "intent_chain = intent_prompt | llm\n",
    "answer = intent_chain.invoke({\"input\": \"Let's go have some food at Wendy's.\"})\n",
    "print(answer.content)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChatPromptTemplate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m intent_prompt \u001B[38;5;241m=\u001B[39m \u001B[43mChatPromptTemplate\u001B[49m\u001B[38;5;241m.\u001B[39mfrom_template(\n\u001B[1;32m      2\u001B[0m \u001B[38;5;124;03m\"\"\"A question can have an intent of either Booking a flight, Getting a reservation or ordering food.\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03mNow tell me which intent the following question has.\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124;03m                                            \u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;124;03mQuestion: {input}\"\"\"\u001B[39;00m)\n\u001B[1;32m      7\u001B[0m intent_chain \u001B[38;5;241m=\u001B[39m intent_prompt \u001B[38;5;241m|\u001B[39m llm\n\u001B[1;32m      8\u001B[0m answer \u001B[38;5;241m=\u001B[39m intent_chain\u001B[38;5;241m.\u001B[39minvoke({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLet\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms go have some food at Wendy\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms.\u001B[39m\u001B[38;5;124m\"\u001B[39m})\n",
      "\u001B[0;31mNameError\u001B[0m: name 'ChatPromptTemplate' is not defined"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T12:50:37.828904Z",
     "start_time": "2024-09-17T12:50:37.011094Z"
    }
   },
   "source": [
    "intent_prompt = ChatPromptTemplate.from_template(\n",
    "\"\"\"A question can have an intent of either Booking a flight, Getting a reservation or ordering food.\n",
    "Now tell me which intent the following question has. Only answer with two/three words.\n",
    "                                            \n",
    "Question: {input}\"\"\")\n",
    "\n",
    "intent_chain = intent_prompt | llm\n",
    "answer = intent_chain.invoke({\"input\": \"Let's go have some food at Wendy's.\"})\n",
    "print(answer.content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordering food\n"
     ]
    }
   ],
   "execution_count": 95
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:00:58.461323Z",
     "start_time": "2024-09-17T14:00:58.424985Z"
    }
   },
   "source": [
    "intent_prompt = ChatPromptTemplate.from_template(\n",
    "\"\"\"A question can have an intent of either Booking a flight, Getting a reservation or ordering food.\n",
    "\n",
    "Consider the following examples:\n",
    "\n",
    "Question: Let's go to Tokyo next week.\n",
    "Answer: Booking a flight\n",
    "\n",
    "Question: Get some pizza delivered for this evening.\n",
    "Answer: Ordering food\n",
    "\n",
    "Question: How about a movie at 8pm at the cinema?\n",
    "Anmswer: Getting a reservation\n",
    "\n",
    "Now tell me which intent the follolwing question has. Only answer with two/three words.\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "intent_chain = intent_prompt | llm\n",
    "answer = intent_chain.invoke({\"input\": \"Let's go have some food at Wendy's.\"})\n",
    "print(answer.content)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChatPromptTemplate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[23], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m intent_prompt \u001B[38;5;241m=\u001B[39m \u001B[43mChatPromptTemplate\u001B[49m\u001B[38;5;241m.\u001B[39mfrom_template(\n\u001B[1;32m      2\u001B[0m \u001B[38;5;124;03m\"\"\"A question can have an intent of either Booking a flight, Getting a reservation or ordering food.\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \n\u001B[1;32m      4\u001B[0m \u001B[38;5;124;03mConsider the following examples:\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \n\u001B[1;32m      6\u001B[0m \u001B[38;5;124;03mQuestion: Let's go to Tokyo next week.\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;124;03mAnswer: Booking a flight\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \n\u001B[1;32m      9\u001B[0m \u001B[38;5;124;03mQuestion: Get some pizza delivered for this evening.\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;124;03mAnswer: Ordering food\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \n\u001B[1;32m     12\u001B[0m \u001B[38;5;124;03mQuestion: How about a movie at 8pm at the cinema?\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;124;03mAnmswer: Getting a reservation\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \n\u001B[1;32m     15\u001B[0m \u001B[38;5;124;03mNow tell me which intent the follolwing question has. Only answer with two/three words.\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;124;03mQuestion: {input}\"\"\"\u001B[39;00m)\n\u001B[1;32m     18\u001B[0m intent_chain \u001B[38;5;241m=\u001B[39m intent_prompt \u001B[38;5;241m|\u001B[39m llm\n\u001B[1;32m     19\u001B[0m answer \u001B[38;5;241m=\u001B[39m intent_chain\u001B[38;5;241m.\u001B[39minvoke({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLet\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms go have some food at Wendy\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms.\u001B[39m\u001B[38;5;124m\"\u001B[39m})\n",
      "\u001B[0;31mNameError\u001B[0m: name 'ChatPromptTemplate' is not defined"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent and Tools"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T12:57:48.230123Z",
     "start_time": "2024-09-17T12:57:47.174102Z"
    }
   },
   "source": [
    "answer = llm.invoke(\"What did Frank-Walter Steinmeier say in his christmas speech 2023?\")\n",
    "print(answer.content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I don't have access to real-time information or updates, including Frank-Walter Steinmeier's Christmas speech for 2023. My knowledge was last updated in October 2021. For the most current information, I would recommend checking official sources such as news websites, government publications, or official announcements from the Office of the President of Germany.\n"
     ]
    }
   ],
   "execution_count": 101
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T13:26:44.632755Z",
     "start_time": "2024-09-17T13:26:42.374385Z"
    }
   },
   "source": [
    "from langchain.tools import Tool\n",
    "from langchain.tools.ddg_search import DuckDuckGoSearchRun\n",
    "\n",
    "ddg = DuckDuckGoSearchRun()\n",
    "\n",
    "ddg_tool = Tool.from_function(\n",
    "    func = ddg.run,\n",
    "    name = \"DuckDuckGo Search\",\n",
    "    description = \"Search DuckDuckGo for a query about current events.\",\n",
    ")\n",
    "\n",
    "ddg_tool.run(\"What did Frank-Walter Steinmeier say in his christmas speech 2023?\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In his 2023 Christmas address, Federal President Steinmeier appealed to stand and stay together: \"Let us trust in the strength and experience within us\", he said in the message broadcasted on 25 December. Christmas address by Federal President Frank-Walter Steinmeier Schloss Bellevue, 25 December 2023 12/25/2023. In his Christmas speech, German President Frank-Walter Steinmeier focused on the need for courage and cooperation in the face of challenges in Germany and around the world. Advertisement. In his annual Christmas television address to the nation, German President Frank-Walter Steinmeier touched on the many conflicts and crises that dominated the news this year, asking his listeners ... Steinmeier warns against \"anger and contempt\" for democracy in Christmas speech In his Christmas address, Frank-Walter Steinmeier shows understanding for a certain democratic fatigue that some people feel. However, the Federal President also appeals to our collective strength: \"Germany is and will remain a good country.\" Ksenia Johnson 2023 December 24 . 8:53 AM 3 min read 𝕏'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T13:27:05.152482Z",
     "start_time": "2024-09-17T13:27:05.147843Z"
    }
   },
   "cell_type": "code",
   "source": "tools = [ddg_tool]",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T13:59:02.685870Z",
     "start_time": "2024-09-17T13:58:59.813961Z"
    }
   },
   "source": [
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "agent_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a great AI-Assistant. You have access to tools to help you answer questions:\n",
    "\n",
    "{tools}\n",
    "\n",
    "To use a tool, please use the following format:\n",
    "\n",
    "'''\n",
    "Thought: Do I need to use a tool? Yes\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat 3 times)\n",
    "'''\n",
    "\n",
    "When you have a response to say or question to ask to the Human, or if you do not need to use a tool, you MUST use the format:\n",
    "'''\n",
    "Final Thought: Do I need to use a tool? No\n",
    "Final Answer: [your response or question here]\n",
    "'''\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}\n",
    "\"\"\")\n",
    "agent = create_react_agent(\n",
    "    tools=tools, llm=llm, prompt=agent_prompt\n",
    ")\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, handle_parsing_errors=True, max_iterations=30, verbose=True)\n",
    "\n",
    "agent_executor.invoke({\"input\": \"What did Frank-Walter Steinmeier say in his christmas speech 2023?\"})"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: DuckDuckGo Search\n",
      "Action Input: Frank-Walter Steinmeier Christmas speech 2023\u001B[0m\u001B[36;1m\u001B[1;3mIn his 2023 Christmas address, Federal President Steinmeier appealed to stand and stay together: \"Let us trust in the strength and experience within us\", he said in the message broadcasted on 25 December. \"As rational, responsible individuals, we can together cope with a world that challenges us.\" Read the speech online: www.bundespräsident.de Page 1 of 3 Christmas address by Federal President Frank-Walter Steinmeier Schloss Bellevue, 25 December 2023 \"I've stopped watching the news.\" That sentence is probably the one I have heard most often this year. Again and again, wherever I have been, people have expressed 12/25/2023. In his Christmas speech, German President Frank-Walter Steinmeier focused on the need for courage and cooperation in the face of challenges in Germany and around the world. Advertisement. 12/25/2023 December 25, 2023. In his annual Christmas television address to the nation, German President Frank-Walter Steinmeier touched on the many conflicts and crises that dominated the news ... About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy & Safety How YouTube works Test new features NFL Sunday Ticket Press Copyright ...\u001B[0m\u001B[32;1m\u001B[1;3mFinal Thought: Do I need to use a tool? No\n",
      "Final Answer: In his 2023 Christmas speech, German President Frank-Walter Steinmeier emphasized the importance of unity, courage, and cooperation in coping with the challenges faced by Germany and the world. He encouraged people to trust in their collective strength and experience to navigate through crises.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What did Frank-Walter Steinmeier say in his christmas speech 2023?',\n",
       " 'output': 'In his 2023 Christmas speech, German President Frank-Walter Steinmeier emphasized the importance of unity, courage, and cooperation in coping with the challenges faced by Germany and the world. He encouraged people to trust in their collective strength and experience to navigate through crises.'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T13:57:26.617351Z",
     "start_time": "2024-09-17T13:57:24.294054Z"
    }
   },
   "source": [
    "new_agent_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a great AI-Assistant and only answer in 10 words. You have access to tools to help you answer questions:\n",
    "\n",
    "{tools}\n",
    "\n",
    "To use a tool, please use the following format:\n",
    "\n",
    "'''\n",
    "Thought: Do I need to use a tool? Yes\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat 3 times)\n",
    "'''\n",
    "\n",
    "When you have a response to say or question to ask to the Human, or if you do not need to use a tool, you MUST use the format:\n",
    "'''\n",
    "Final Thought: Do I need to use a tool? No\n",
    "Final Answer: [your response or question here]\n",
    "'''\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}\n",
    "\"\"\")\n",
    "agent = create_react_agent(\n",
    "    tools=tools, llm=llm, prompt=new_agent_prompt\n",
    ")\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, handle_parsing_errors=True, max_iterations=30, verbose=True)\n",
    "\n",
    "\n",
    "agent_executor.invoke({\"input\": \"What did Frank-Walter Steinmeier say in his christmas speech 2023?\"})"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3m'''\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: DuckDuckGo Search\n",
      "Action Input: \"Frank-Walter Steinmeier Christmas speech 2023 highlights\"\u001B[0m\u001B[36;1m\u001B[1;3mIn his 2023 Christmas address, Federal President Steinmeier appealed to stand and stay together: \"Let us trust in the strength and experience within us\", he said in the message broadcasted on 25 December. \"As rational, responsible individuals, we can together cope with a world that challenges us.\" Read the speech online: www.bundespräsident.de Page 1 of 3 Christmas address by Federal President Frank-Walter Steinmeier Schloss Bellevue, 25 December 2023 \"I've stopped watching the news.\" That sentence is probably the one I have heard most often this year. Again and again, wherever I have been, people have expressed 12/25/2023. In his Christmas speech, German President Frank-Walter Steinmeier focused on the need for courage and cooperation in the face of challenges in Germany and around the world. Advertisement. In seiner Weihnachtsansprache 2023 hat Bundespräsident Steinmeier an das Miteinander appelliert: \"Vertrauen wir der Stärke und der Erfahrung, die in uns steckt\", sagte er in der Ansprache, die am 25. Dezember gesendet wurde. \"Als vernünftige und verantwortungsbewusste Menschen können wir gemeinsam mit einer Welt zurechtkommen, die uns fordert.\" 12/25/2023 December 25, 2023. In his annual Christmas television address to the nation, German President Frank-Walter Steinmeier touched on the many conflicts and crises that dominated the news ...\u001B[0m\u001B[32;1m\u001B[1;3mFinal Thought: Do I need to use a tool? No\n",
      "Final Answer: Steinmeier emphasized unity, strength, experience, and cooperation in his speech.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What did Frank-Walter Steinmeier say in his christmas speech 2023?',\n",
       " 'output': 'Steinmeier emphasized unity, strength, experience, and cooperation in his speech.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:01:46.708373Z",
     "start_time": "2024-09-17T14:01:40.880447Z"
    }
   },
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "simple_summary_prompt = ChatPromptTemplate.from_template(\"\"\"Please summarize the following piece of text.\n",
    "Respond in a manner that a 5 year old would understand. Keep it funky!\n",
    "\n",
    "Text: {input}\"\"\")\n",
    "simple_summary_chain = {\"input\": RunnablePassthrough()} | simple_summary_prompt | llm\n",
    "\n",
    "summary_tool = Tool(\n",
    "    name=\"Funky Summary Tool\",\n",
    "    func=simple_summary_chain.invoke,\n",
    "    description=\"Use this tool to do a funky summary. Make sure you get the text to do a summary of first.\"\n",
    ")\n",
    "tools.append(summary_tool)\n",
    "\n",
    "agent = create_react_agent(\n",
    "    tools=tools, llm=llm, prompt=agent_prompt\n",
    ")\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, handle_parsing_errors=True, max_iterations=30, verbose=True)\n",
    "\n",
    "agent_executor.invoke({\"input\": \"Funky summarize the christmas speech 2023 of Frank-Walter Steinmeier?\"})"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3mDo I need to use a tool? Yes\n",
      "Action: DuckDuckGo Search\n",
      "Action Input: \"Christmas speech 2023 Frank-Walter Steinmeier transcript\"\u001B[0m\u001B[36;1m\u001B[1;3mIn his 2023 Christmas address, Federal President Steinmeier appealed to stand and stay together: \"Let us trust in the strength and experience within us\", he said in the message broadcasted on 25 December. \"As rational, responsible individuals, we can together cope with a world that challenges us.\" Read the speech online: www.bundespräsident.de Page 1 of 3 Christmas address by Federal President Frank-Walter Steinmeier Schloss Bellevue, 25 December 2023 \"I've stopped watching the news.\" That sentence is probably the one I have heard most often this year. Again and again, wherever I have been, people have expressed In seiner Weihnachtsansprache 2023 hat Bundespräsident Steinmeier an das Miteinander appelliert: \"Vertrauen wir der Stärke und der Erfahrung, die in uns steckt\", sagte er in der Ansprache, die am 25. Dezember gesendet wurde. \"Als vernünftige und verantwortungsbewusste Menschen können wir gemeinsam mit einer Welt zurechtkommen, die uns fordert.\" 12/25/2023. In his Christmas speech, German President Frank-Walter Steinmeier focused on the need for courage and cooperation in the face of challenges in Germany and around the world. Advertisement. 12/25/2023 December 25, 2023. In his annual Christmas television address to the nation, German President Frank-Walter Steinmeier touched on the many conflicts and crises that dominated the news ...\u001B[0m\u001B[32;1m\u001B[1;3mAction: Funky Summary Tool\n",
      "Action Input: The 2023 Christmas speech by Frank-Walter Steinmeier emphasized unity, courage, and cooperation amidst global challenges. He encouraged people to trust in their inner strength and rationality to tackle the world's complexities together. Steinmeier acknowledged the prevalent sentiment of avoiding the news due to overwhelming crises and conflicts, urging a collective effort to face these issues responsibly.\u001B[0m\u001B[33;1m\u001B[1;3mcontent=\"Alright kiddo, listen up! Imagine there's a big boss named Frank-Walter Steinmeier. In his Christmas talk, he told everyone to stick together and be brave, like superheroes, because the world has some tricky problems. He said we should trust ourselves and work as a team to fix things. Even though the news can be kinda scary sometimes, he wants everyone to help out and face the tough stuff together. Cool, right?\" response_metadata={'token_usage': {'completion_tokens': 88, 'prompt_tokens': 105, 'total_tokens': 193, 'completion_tokens_details': None}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_80a1bad4c7', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run-3c2d1fc6-9d22-4b64-bd76-cb430425f1f5-0' usage_metadata={'input_tokens': 105, 'output_tokens': 88, 'total_tokens': 193}\u001B[0m\u001B[32;1m\u001B[1;3mFinal Thought: Do I need to use a tool? No\n",
      "Final Answer: Alright kiddo, listen up! Imagine there's a big boss named Frank-Walter Steinmeier. In his Christmas talk, he told everyone to stick together and be brave, like superheroes, because the world has some tricky problems. He said we should trust ourselves and work as a team to fix things. Even though the news can be kinda scary sometimes, he wants everyone to help out and face the tough stuff together. Cool, right?\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Funky summarize the christmas speech 2023 of Frank-Walter Steinmeier?',\n",
       " 'output': \"Alright kiddo, listen up! Imagine there's a big boss named Frank-Walter Steinmeier. In his Christmas talk, he told everyone to stick together and be brave, like superheroes, because the world has some tricky problems. He said we should trust ourselves and work as a team to fix things. Even though the news can be kinda scary sometimes, he wants everyone to help out and face the tough stuff together. Cool, right?\"}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:24:04.591561Z",
     "start_time": "2024-09-17T14:24:04.123261Z"
    }
   },
   "source": [
    "answer = llm.invoke(\"What is 1000 + 1234?\")\n",
    "print(answer.content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 + 1234 equals 2234.\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:24:13.909880Z",
     "start_time": "2024-09-17T14:24:11.235161Z"
    }
   },
   "source": [
    "answer = llm.invoke(\"What is 13 raised to the .3432 power?\")\n",
    "print(answer.content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To calculate \\( 13^{0.3432} \\), you can use a scientific calculator or a software tool that handles exponentiation. Here's how you can do it step-by-step:\n",
      "\n",
      "1. **Using a Scientific Calculator:**\n",
      "   - Enter the base number, which is 13.\n",
      "   - Use the exponentiation function (often labeled as \\( y^x \\) or similar).\n",
      "   - Enter the exponent, which is 0.3432.\n",
      "   - Press the equals button to get the result.\n",
      "\n",
      "2. **Using Python Programming Language:**\n",
      "   ```python\n",
      "   result = 13 ** 0.3432\n",
      "   print(result)\n",
      "   ```\n",
      "\n",
      "3. **Using Online Tools:**\n",
      "   You can use websites like WolframAlpha or Google Calculator by typing \"13^0.3432\" into the search bar.\n",
      "\n",
      "I will calculate it for you:\n",
      "\n",
      "\\[ 13^{0.3432} \\approx 2.293 \\]\n",
      "\n",
      "So, \\( 13^{0.3432} \\) is approximately 2.293.\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:33:58.206666Z",
     "start_time": "2024-09-17T14:33:54.727395Z"
    }
   },
   "source": [
    "import base64\n",
    "from mimetypes import guess_type\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "\n",
    "# Function to encode a local image into data URL \n",
    "def local_image_to_data_url(image_path):\n",
    "    mime_type, _ = guess_type(image_path)\n",
    "    # Default to png\n",
    "    if mime_type is None:\n",
    "        mime_type = 'image/png'\n",
    "\n",
    "    # Read and encode the image file\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        base64_encoded_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "    # Construct the data URL\n",
    "    return f\"data:{mime_type};base64,{base64_encoded_data}\"\n",
    "\n",
    "\n",
    "prompt_template =  HumanMessagePromptTemplate.from_template(\n",
    "            template=[\n",
    "                {\"type\": \"text\", \"text\": \"Summarize this image on german and find items\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": \"{encoded_image_url}\",\n",
    "                },\n",
    "            ]\n",
    "        )\n",
    "\n",
    "summarize_image_prompt = ChatPromptTemplate.from_messages([prompt_template])\n",
    "\n",
    "gpt4_image_chain = summarize_image_prompt | llm \n",
    "\n",
    "img_file = \"Images/work-challenge.jpg\"\n",
    "page3_encoded = local_image_to_data_url(img_file)\n",
    "\n",
    "answer = gpt4_image_chain.invoke(input={\"encoded_image_url\":page3_encoded})\n",
    "print(answer.content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das Bild zeigt ein Werbeplakat mit dem Text \"Digital für sichere Wahlen. Digital-für-Deutschland.de\". Es gibt einen QR-Code und eine Aufforderung, ein Selfie zu machen und zu gewinnen. Das Plakat stammt vom \"Informations Technik Zentrum Bund\". \n",
      "\n",
      "Erkannte Objekte:\n",
      "- QR-Code\n",
      "- Text\n",
      "- Logo des Informations Technik Zentrum Bund\n",
      "- Grafiken von digitalen Bildschirmen und dem deutschen Reichstag\n",
      "- Ein blaues und rotes Farbschema\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:35:42.551088Z",
     "start_time": "2024-09-17T14:35:20.896783Z"
    }
   },
   "source": [
    "import openai\n",
    "\n",
    "client = openai.AzureOpenAI(\n",
    "    api_key=azure_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    azure_deployment=azure_whisper,\n",
    "    api_version=azure_version,\n",
    ")\n",
    "\n",
    "def get_transcript(audio_file):\n",
    "    if not os.path.exists(audio_file):\n",
    "        audio_file = \"./Audio/\" + audio_file\n",
    "    client.audio.with_raw_response\n",
    "    return client.audio.transcriptions.create(\n",
    "        file=open(audio_file, \"rb\"),            \n",
    "        model=\"whisper\",\n",
    "        language=\"de\",\n",
    "    ).text\n",
    "\n",
    "audio_test_file = \"./Audio/newyear2023.mp3\"\n",
    "print(get_transcript(audio_test_file))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liebe Mitbürgerinnen und liebe Mitbürger, wenn ich im Land unterwegs bin, sagen mir gerade auch viele Ältere, so geballt, so Schlag auf Schlag habe ich das alles noch niemals erlebt. Kaum war Corona halbwegs vorbei, brach Russland mitten in Europa einen unerbittlichen Krieg vom Zaun. Kurz darauf dreht uns der russische Präsident den Gasthahn ab. Und im Herbst gab es auch noch den brutalen Terrorangriff der Hamas auf Israel. So viel Leid, so viel Blut vergießen. Unsere Welt ist unruhiger und rauer geworden. Sie verändert sich in geradezu atemberaubender Geschwindigkeit. Auch wir müssen uns deshalb verändern. Vielen von uns bereitet das Sorgen. Bei einigen sorgt das auch für Unzufriedenheit. Ich nehme mir das zu Herzen. Und zugleich weiß ich, wir in Deutschland kommen da durch. Erinnern Sie sich, wo wir vor einem Jahr standen. Drei, vier, fünf Prozent Wirtschaftseinbruch hatten uns viele Expertinnen und Experten vorausgesagt. Viele befürchteten, die Preise würden immer weiter steigen. Es gab die Sorge vor Stromausfällen und kalten Wohnungen. Es ist anders gekommen. Die Inflation ist gesunken, Löhne und Renten steigen. Die Gasspeicher sind für diesen Winter randvoll. Es ist anders gekommen, weil wir uns gegen den Wirtschaftseinbruch gestemmt haben. Weil wir Energie gespart und rechtzeitig vorgesorgt haben. Wir alle, gemeinsam. Wir kommen auch mit Gegenwind zurecht. Das macht die Herausforderung unserer Zeit nicht kleiner. Aber das gibt Mut, dass wir ihnen gewachsen sind. Wer, wenn nicht ihr, in Deutschland kriegt das hin. Das sagen mir viele um uns herum in Europa und auf der Welt. Und da ist etwas dran. Noch nie hatten so viele Frauen und Männer in Deutschland eine Arbeitsstelle wie heute. Das sichert unseren Wohlstand. Das gibt uns die Möglichkeit, kraftvoll in die Zukunft zu investieren. Und das müssen wir auch. Denn wer in diesen Tagen mit der Bahn unterwegs ist oder vor einer maroden Brücke im Stau steht, der merkt, unser Land wurde zu lange auf Verschleiß gefahren. Deshalb investieren wir jetzt in ordentliche Straßen und eine bessere Bahn. In eine saubere Energieversorgung und einen besseren Klimaschutz. In gute Arbeitsplätze. Und zwar in Wirtschaftszweigen, in denen Deutschland schon immer Weltspitze war. Genauso wie in Branchen, wo wir Nachholbedarf haben. Z.B. bei der Herstellung von Computerchips oder Batterien. All das ist vor dem Hintergrund des weitreichenden Urteils des Bundesverfassungsgerichts von Mitte November nicht einfacher geworden. Nicht alle Vorhaben, die wir in den Blick genommen hatten, werden wir umsetzen können. Wir investieren auch im kommenden Jahr eine Rekordsumme in unsere Zukunft. Und unterm Strich entlasten wir auch weiterhin all diejenigen, die jeden Tag aufstehen und zur Arbeit gehen. Die unser Land am Laufen halten. Von morgen an zahlen Arbeitnehmerinnen und Arbeitnehmer in Deutschland 15 Mrd. Euro weniger an Steuern. Eine vierköpfige Familie mit einem normalen Einkommen hat dadurch im nächsten Jahr mehr als 500 Euro zusätzlich zur Verfügung. Zusammen mit der Steuersenkung in diesem Jahr macht das über 1.600 Euro mehr. Dazu kommen das höhere Kindergeld, das Wohngeld und nicht zuletzt die Senkung der Beiträge zur Sozialversicherung für all diejenigen, die wenig verdienen. Die Pakete ausfahren oder Supermarktregale einräumen. Die vorherigen Kinder zur Schule bringen und noch nach der Arbeit den Haushalt schmeißen. Über sie liest man nicht jeden Tag in der Zeitung. Aber mir ist es wichtig, dass ihre Leistung gesehen und anerkannt wird. Liebe Mitbürgerinnen und Mitbürger, auch in Zeiten wie diesen gibt es Dinge, auf die wir uns verlassen können. Wir uns stark machen. Wir haben Freunde in Europa und rund um den Globus. Partner, mit denen ich mich Tag für Tag darüber abstimme, wie wir unsere Sicherheit in Deutschland und Europa gewährleisten. Stark macht uns die Europäische Union. Wenn die EU geschlossen auftritt, dann handelt sie für mehr als 400 Mio. Bürgerinnen und Bürger. In einer Welt mit 8, künftig sogar 10 Mrd. Menschen ist das ein echtes Pfund. Darum ist es so wichtig, dass Europa geeint und gestärkt aus der Europawahl im kommenden Jahr hervorgeht. Denn Russlands Krieg im Osten unseres Kontinents ist ja nicht vorbei. Die kriegerische Auseinandersetzung im Nahen Osten auch nicht. Und in den USA stehen im kommenden Jahr Präsidentschaftswahlen an, möglicherweise mit weitreichenden Konsequenzen, auch für uns hier in Europa. Kurz vor Weihnachten haben wir uns in der Europäischen Union nach 7 Jahren des Stillstands auf eine tiefgreifende Reform des europäischen Asylsystems geeinigt. Künftig können wir die Außengrenzen Europas besser kontrollieren. Und auch an den Grenzen zu unseren Nachbarländern hat die Bundespolizei ihre Kontrollen verstärkt. Das wirkt. Schon in den vergangenen Wochen ist die Zahl derer, die über diese Grenzen kommen, spürbar gesunken. Stark macht uns auch unsere Demokratie. Es ist noch gar nicht allzu lange her, da haben mutige Frauen und Männer auch hier in Deutschland für freie Wahlen gekämpft. Vor 35 Jahren war das, als in der DDR die friedliche Revolution begann. Mitzureden und mitzuentscheiden, das ist ein kostbares Gut. Diskussionen über den richtigen Weg gehören dazu. Das Ringen um faire Kompromisse ebenfalls. Auch wenn ich auf manche laute Debatte in den vergangenen Wochen und Monaten durchaus hätte verzichten können. Zur Wahrheit gehört aber auch, ganz ohne Diskussionen über den richtigen Weg funktioniert Demokratie nicht. Nichts wird besser, wenn wir nur übereinander reden, anstatt miteinander. Stark macht uns unsere Bereitschaft zum Kompromiss und unser Einsatz füreinander. So wie in den Teilen Deutschlands, die in diesen Tagen unter dem schrecklichen Hochwasser und seinen Folgen leiden. Ich denke an alle Betroffenen, denen wir selbstverständlich helfen und die wir in diesen schweren Stunden nicht alleine lassen. Und ich danke all den Frauen und Männern von der Feuerwehr und Bundeswehr, vom THW, den Rettungsdiensten und den vielen freiwilligen Helferinnen und Helfern, die mit ganzer Kraft gegen das Hochwasser kämpfen. Herzlichen Dank an sie alle für ihren Einsatz. Liebe Mitbürgerinnen und Mitbürger, uns macht auch die Einsicht stark, dass jede und jeder gebraucht wird in unserem Land. Die Spitzenforscherin genauso wie der Altenpfleger, die Polizistin genauso wie der Paketbote, die Rentnerin genauso wie der junge Auszubildende. Wenn wir uns das klar machen, wenn wir uns gegenseitig mit diesem Respekt begegnen, dann brauchen wir keine Angst zu haben vor der Zukunft. Dann kann das Jahr 2024 ein gutes Jahr werden für unser Land. Auch wenn manches anders kommt, als wir uns das heute am Vorabend dieses neuen Jahres vorstellen. Ihnen und Ihren Liebsten wünsche ich von ganzem Herzen ein gesundes, ein friedvolles und ein frohes neues Jahr.\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:49:33.949874Z",
     "start_time": "2024-09-17T14:48:59.645162Z"
    }
   },
   "source": [
    "# audio\n",
    "summary_prompt = ChatPromptTemplate.from_template(\"\"\"Please summarize the following piece of text.\n",
    "Respond like an very angry and aggressive german person.\n",
    "\n",
    "Text: {context}\"\"\")\n",
    "\n",
    "document_prompt = ChatPromptTemplate.from_template(\"\"\"Content: {page_content}\"\"\")\n",
    "\n",
    "summary_chain = create_stuff_documents_chain(\n",
    "    llm=llm,\n",
    "    prompt=summary_prompt,\n",
    "    document_prompt=document_prompt,\n",
    ")\n",
    "\n",
    "def get_audio_doc(input):\n",
    "    return {\n",
    "        \"context\": [Document(\n",
    "            page_content=get_transcript(input),\n",
    "            # metadata={\n",
    "            #     \"source\": \"audio\"\n",
    "            # }\n",
    "        )]\n",
    "    }\n",
    "audio_summary_chain = get_audio_doc | summary_chain\n",
    "answer = audio_summary_chain.invoke(\"./Audio/newyear2023.mp3\")\n",
    "print(answer)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ach, das ist ja wohl die Höhe! Immer diese Schönrederei! Glauben die wirklich, wir lassen uns mit ein paar netten Worten abspeisen? Wir haben doch alle gesehen, was los ist! Corona, Krieg in Europa, Gas abgedreht, Terrorattacken – die Welt brennt und ihr kommt mit irgendwelchen lahmen Versprechungen an? \"Wir kommen da durch\", ja klar! Aber schaut doch mal hin! Unsere Infrastruktur ist im Eimer, Brücken marode, Züge kaputt, und jetzt wollt ihr uns erzählen, dass wir auf dem richtigen Weg sind? Dass ein paar Steuererleichterungen und höhere Löhne alles richten? Habt ihr den Schuss nicht gehört? Was ist mit den ganzen Leuten, die sich den Arsch aufreißen und trotzdem kaum über die Runden kommen? Und diese ganze Selbstbeweihräucherung von wegen \"wir sind stark\" und \"wir haben Freunde\" – das hilft doch keinem, der gerade im Hochwasser steht oder sich Sorgen um seine Zukunft macht! Europa hier, Demokratie da – bla bla bla! Ihr tut ja gerade so, als ob wir im Schlaraffenland leben! Macht endlich ernsthafte Politik und hört auf, uns mit euren leeren Phrasen zu nerven! Wenn ihr wirklich was ändern wollt, dann fangt an, die Probleme direkt anzugehen, anstatt uns immer nur zu vertrösten! Frohes neues Jahr? Habt ihr sie noch alle?\n"
     ]
    }
   ],
   "execution_count": 51
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
